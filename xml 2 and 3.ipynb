{
 "metadata": {
  "name": "",
  "signature": "sha256:0053e92b14699d7a282384a985a16ec099047b50aff4281a8fe0e5ae662dbf24"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree, objectify\n",
      "from io import StringIO, BytesIO\n",
      "import pandas as pd\n",
      "\n",
      "tree = etree.parse(\"14-28-RAW-Solr-2_toparse.xml\")\n",
      "root2 = tree.getroot()\n",
      "tree = etree.parse(\"14-28-RAW-Solr-3b_toparse.xml\")\n",
      "root3b = tree.getroot()\n",
      "\n",
      "list2 = []\n",
      "list3 = []\n",
      "\n",
      "for doc in root2.findall('doc'):\n",
      "    list2.append(dict([(i.values()[0], i.getchildren()[0].text) for i in doc.getchildren() if i.values()[0]!='score']))\n",
      "for doc in root3b.findall('doc'):\n",
      "    list3.append(dict([(i.values()[0], i.getchildren()[0].text) for i in doc.getchildren() if i.values()[0]!='score']))\n",
      "\n",
      "dates2 = [d['dateRcpt'] if 'dateRcpt' in d else None for d in list2]\n",
      "states2 = [d['stateCd'] if 'stateCd' in d else None for d in list2]\n",
      "comments2 = [d['text'] if 'text' in d else None for d in list2]\n",
      "dates3 = [d['dateRcpt'] if 'dateRcpt' in d else None for d in list3]\n",
      "states3 = [d['stateCd'] if 'stateCd' in d else None for d in list3]\n",
      "comments3 = [d['text'] if 'text' in d else None for d in list3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
      "from time import time\n",
      "import os, os.path\n",
      "import csv\n",
      "from bunch import Bunch\n",
      "import numpy as np\n",
      "import re\n",
      "import string\n",
      "# For strip/clean data. Replace words w/root form, \"clean\" rids html\n",
      "from nltk.corpus import wordnet as wn\n",
      "from nltk import word_tokenize, regexp_tokenize, clean_html\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem import PorterStemmer\n",
      "#from langid import classify\n",
      "from sklearn.datasets import load_files\n",
      "import pandas as pd\n",
      "from sklearn.metrics.pairwise import cosine_similarity  \n",
      "from nltk.stem.snowball import SnowballStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "## *************************     Tokenizer      *******************************\n",
      "# for a list of strings\n",
      "class CleanLemmaTokenizer(object):  # Returns root form, AND cleans HTML\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "        \n",
      "class SynsetReplacer(object):\n",
      "    def replace(self, word, pos=None):\n",
      "        synset=wn.synsets(word)  #Ordered by commonality, so 1st is best\n",
      "        if len(synset) >= 1:\n",
      "            return synset[0].name  ## first synonym is the most common\n",
      "        else:\n",
      "            return word   \n",
      "            \n",
      "class CleanLemmaSynsetTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "        self.replacer = SynsetReplacer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.replacer.replace(self.wnl.lemmatize(t)) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "        \n",
      "class CleanPorterTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.ps = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.ps.stem(t) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "\n",
      "class CleanSnowballTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.ps = SnowballStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.ps.stem(t) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "\n",
      "## *************************    VECTORIZE DATA      *******************************\n",
      "# FOR GETTING Feature words: print '\\', \\''.join([features[i] for i in PS]) \n",
      "def preprocess_num(data):\n",
      "    for i in range(len(data)):\n",
      "        data[i]=data[i].lower()\n",
      "        #data[i]=data[i].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "        data[i]=re.sub('\\s[2][0-1][0-9][0-9]\\s', ' YEAR_tag ', data[i])\n",
      "        data[i]=re.sub('\\s[0-9]+\\s', ' NUM_tag ', data[i])\n",
      "        data[i]=re.sub('\\s([0-9]|[a-z])([0-9]|[a-z])+[0-9]([0-9]|[a-z])+\\s', ' SERIAL_tag ', data[i])\n",
      "        \n",
      "# def preprocess(data):\n",
      "    # PS_WORDS=frozenset(['appliance', 'battery', 'blade', 'bought', 'broken', 'bulb', 'buy', 'car', 'cost', 'dishwasher', 'dispenser', 'door', 'drill', 'dryer', 'filter', 'freezer', 'fridge', 'glass', 'guarantee', 'handle', 'hole','hose', 'ice', 'instruction', 'iron', 'item', 'light', 'machine', 'manual', 'model', 'new', 'order', 'oven', 'problem', 'product', 'purchase', 'refrigerator', 'replace', 'replacement', 'spare', 'spin', 'spring', 'tool', 'used', 'warranty', 'wash', 'washer', 'washing', 'water'])\n",
      "    # HR_WORDS=frozenset(['application', 'apply', 'bachelor', 'business', 'career', 'college', 'cv', 'department', 'design', 'employment', 'engineer', 'engineering', 'experience', 'german', 'germany', 'graduate','growth', 'hr', 'industry','internship', 'job', 'join', 'junior', 'madam', 'management', 'manager', 'master', 'month', 'opening', 'opportunity', 'organisation', 'phd', 'placement', 'position', 'process', 'profile', 'program', 'project', 'recruitment', 'related', 'resource', 'resume', 'skill','student', 'study', 'studying', 'team', 'trainee', 'training', 'university', 'wish'])\n",
      "    # SALUTATION_WORDS=frozenset(['hi','hello','dear','bye','thank', 'thanks','best', 'cheers'])\n",
      "    # PS_regex = re.compile('\\s('+'|'.join(PS_WORDS)+')\\s')\n",
      "    # HR_regex = re.compile('\\s('+'|'.join(HR_WORDS)+')\\s')\n",
      "    # SAL_regex = re.compile('\\s('+'|'.join(HR_WORDS)+')\\s')\n",
      "    # wnl = WordNetLemmatizer()\n",
      "    # for i in range(len(data)):\n",
      "        # data[i]=data[i].lower()\n",
      "        # data[i]=data[i].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "        # data[i]=' '.join([wnl.lemmatize(tok) for tok in data[i].split()])\n",
      "        # data[i]=re.sub('\\s[2][0-1][0-9][0-9]\\s', ' YEAR_tag ', data[i])\n",
      "        # data[i]=re.sub('\\s[0-9]+\\s', ' NUM_tag ', data[i])\n",
      "        # data[i]=re.sub('\\s([0-9]|[a-z])([0-9]|[a-z])+[0-9]([0-9]|[a-z])+\\s', ' SERIAL_tag ', data[i])\n",
      "        # data[i]=re.sub(PS_regex, ' PS_tag ', data[i])\n",
      "        # data[i]=re.sub(HR_regex, ' HR_tag ', data[i])\n",
      "        # data[i]=re.sub(SAL_regex, ' SAL_tag ', data[i])\n",
      "    \n",
      "def fit_vectorizer(data):    \n",
      "    print(\"Creating Vectorizer...\")\n",
      "    t0 = time()\n",
      "    myTokenizer=CleanPorterTokenizer()   # Select the Tokenizer to use\n",
      "    #myTokenizer=CleanLemmaSynsetTokenizer() \n",
      "    #myTokenizer=CleanPorterTokenizer() \n",
      "    vectorizer = TfidfVectorizer(max_features=2000, tokenizer=myTokenizer,\n",
      "        stop_words='english', use_idf=True, binary=False, charset_error='ignore',  max_df=.99)\n",
      "    vectorizer.fit(data)\n",
      "    print(\"done in %fs\" % (time() - t0))\n",
      "    return vectorizer\n",
      "\n",
      "def fit_count_vectorizer(data):\n",
      "    print(\"Creating Count Vectorizer...\")\n",
      "    t0 = time()\n",
      "    myTokenizer = CleanLemmaTokenizer()\n",
      "    vectorizer = CountVectorizer(max_features = 2000, tokenizer = myTokenizer,\n",
      "                                 stop_words = 'english', binary =True, charset_error='ignore', max_df =.99)\n",
      "    vectorizer.fit(data)\n",
      "    print(\"done in %fs\" %(time() - t0))\n",
      "    return vectorizer\n",
      "\n",
      "## *************************    Group Features      *******************\n",
      "def category_features(X, target, target_val, vectorizer):\n",
      "    indices = [i for i, x in enumerate(target) if x == target_val]\n",
      "    X_target_sums=np.sum(X[indices], axis=0)\n",
      "    features=vectorizer.get_feature_names()\n",
      "    # for i, x in sorted(enumerate(X_target_sums), key=lambda tup:tup[1], reverse=True)[:30]:\n",
      "        # print features[i], x\n",
      "    top_features=[i for i,x in sorted(enumerate(X_target_sums), key=lambda tup:tup[1], reverse=True)]\n",
      "    return top_features\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = fit_vectorizer(comments2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating Vectorizer...\n",
        "done in 203.769823s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/genevievekathleensmith/Documents/insight/env/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:625: DeprecationWarning: The charset_error parameter is deprecated as of version 0.14 and will be removed in 0.16. Use decode_error instead.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term_matrix = vectorizer.transform(comments2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dense = term_matrix.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dense_df = pd.DataFrame(dense)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_matrix = dense_df.drop_duplicates()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_series = pd.Series(unique_matrix.index)\n",
      "unique_series.to_csv('unique_ids_2.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "float(len(unique_series))/len(comments2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "0.9626414075237305"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#type(comments3)\n",
      "comments3_text = [d if type(d)==str else None for d in comments3]\n",
      "\n",
      "#type(comments3[0])==str\n",
      "len(comments3),len(comments3_text)\n",
      "#comments3[0],comments3_text[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "(35000, 35000)"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer3 = fit_vectorizer(comments3_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'NoneType' object has no attribute 'lower'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-44-264bccaac82a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments3_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-4-3f69cc4413ad>\u001b[0m in \u001b[0;36mfit_vectorizer\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     72\u001b[0m     vectorizer = TfidfVectorizer(max_features=2000, tokenizer=myTokenizer,\n\u001b[1;32m     73\u001b[0m         stop_words='english', use_idf=True, binary=False, charset_error='ignore',  max_df=.99)\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done in %fs\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/genevievekathleensmith/Documents/insight/env/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \"\"\"\n\u001b[0;32m-> 1250\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/genevievekathleensmith/Documents/insight/env/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/genevievekathleensmith/Documents/insight/env/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    739\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/genevievekathleensmith/Documents/insight/env/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 233\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/genevievekathleensmith/Documents/insight/env/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term_matrix3 = vectorizer3.transform(comments3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dense3 = term_matrix3.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dense3_df = pd.DataFrame(dense3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_matrix3 = dense3_df.drop_duplicates()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_series3 = pd.Series(unique_matrix3.index)\n",
      "unique_series3.to_csv('unique_ids_3.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "#!/usr/bin/python \n",
      "#\n",
      "# (originally entered at https://gist.github.com/1035399)\n",
      "#\n",
      "# License: GPLv3\n",
      "#\n",
      "# To download the AFINN word list do:\n",
      "# wget http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip\n",
      "# unzip imm6010.zip\n",
      "#\n",
      "# Note that for pedagogic reasons there is a UNICODE/UTF-8 error in the code.\n",
      "\n",
      "import math\n",
      "import re\n",
      "import sys\n",
      "reload(sys)\n",
      "sys.setdefaultencoding('utf-8')\n",
      "import simplejson\n",
      "\n",
      "# AFINN-111 is as of June 2011 the most recent version of AFINN\n",
      "filenameAFINN = 'AFINN-111.txt'\n",
      "afinn = dict(map(lambda (w, s): (w, int(s)), [ \n",
      "            ws.strip().split('\\t') for ws in open(filenameAFINN) ]))\n",
      "\n",
      "# Word splitter pattern\n",
      "pattern_split = re.compile(r\"\\W+\")\n",
      "\n",
      "def sentiment(text):\n",
      "    \"\"\"\n",
      "    Returns a float for sentiment strength based on the input text.\n",
      "    Positive values are positive valence, negative value are negative valence. \n",
      "    \"\"\"\n",
      "    words = pattern_split.split(text.lower())\n",
      "    sentiments = map(lambda word: afinn.get(word, 0), words)\n",
      "    if sentiments:\n",
      "        # How should you weight the individual word sentiments? \n",
      "        # You could do N, sqrt(N) or 1 for example. Here I use sqrt(N)\n",
      "        sentiment = float(sum(sentiments))/math.sqrt(len(sentiments))\n",
      "        \n",
      "    else:\n",
      "        sentiment = 0\n",
      "    return sentiment\n",
      "\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    # Single sentence example:\n",
      "    text = \"Finn is stupid and idiotic\"\n",
      "    print(\"%6.2f %s\" % (sentiment(text), text))\n",
      "    \n",
      "    # No negation and booster words handled in this approach\n",
      "    text = \"Finn is only a tiny bit stupid and not idiotic\"\n",
      "    print(\"%6.2f %s\" % (sentiment(text), text))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sents2 = [sentiment(d) for d in comments2]\n",
      "myarray2 = np.asarray(sents2)\n",
      "state_array2 = np.asarray(states2) \n",
      "sent_state2 = pd.DataFrame(state_array2,myarray2)\n",
      "sent_state2.to_csv('sent2.csv')\n",
      "\n",
      "\n",
      "#sents3 = [sentiment(d) for d in comments3]\n",
      "#myarray3 = np.asarray(sents3)\n",
      "#state_array3 = np.asarray(states3) \n",
      "#sent_state3 = pd.DataFrame(state_array3,myarray3)\n",
      "#sent_state3.to_csv('sent3.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "date2_array = np.asarray(dates2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "date2_df = pd.DataFrame(date2_array)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "date2_df.to_csv('date2.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}