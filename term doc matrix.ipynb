{
 "metadata": {
  "name": "",
  "signature": "sha256:0a63a93659e334eb10149933e0aaaa67dcb654a8d4a3d619c34475c5ad978fca"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
      "from time import time\n",
      "import os, os.path\n",
      "import csv\n",
      "from bunch import Bunch\n",
      "import numpy as np\n",
      "import re\n",
      "import string\n",
      "# For strip/clean data. Replace words w/root form, \"clean\" rids html\n",
      "from nltk.corpus import wordnet as wn\n",
      "from nltk import word_tokenize, regexp_tokenize, clean_html\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem import PorterStemmer\n",
      "#from langid import classify\n",
      "from sklearn.datasets import load_files\n",
      "import pandas as pd\n",
      "from sklearn.metrics.pairwise import cosine_similarity  \n",
      "from nltk.stem.snowball import SnowballStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "## *************************     Tokenizer      *******************************\n",
      "# for a list of strings\n",
      "class CleanLemmaTokenizer(object):  # Returns root form, AND cleans HTML\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "        \n",
      "class SynsetReplacer(object):\n",
      "    def replace(self, word, pos=None):\n",
      "        synset=wn.synsets(word)  #Ordered by commonality, so 1st is best\n",
      "        if len(synset) >= 1:\n",
      "            return synset[0].name  ## first synonym is the most common\n",
      "        else:\n",
      "            return word   \n",
      "            \n",
      "class CleanLemmaSynsetTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "        self.replacer = SynsetReplacer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.replacer.replace(self.wnl.lemmatize(t)) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "        \n",
      "class CleanPorterTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.ps = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.ps.stem(t) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "\n",
      "class CleanSnowballTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.ps = SnowballStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.ps.stem(t) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "\n",
      "## *************************    VECTORIZE DATA      *******************************\n",
      "# FOR GETTING Feature words: print '\\', \\''.join([features[i] for i in PS]) \n",
      "def preprocess_num(data):\n",
      "    for i in range(len(data)):\n",
      "        data[i]=data[i].lower()\n",
      "        data[i]=data[i].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "        data[i]=re.sub('\\s[2][0-1][0-9][0-9]\\s', ' YEAR_tag ', data[i])\n",
      "        data[i]=re.sub('\\s[0-9]+\\s', ' NUM_tag ', data[i])\n",
      "        data[i]=re.sub('\\s([0-9]|[a-z])([0-9]|[a-z])+[0-9]([0-9]|[a-z])+\\s', ' SERIAL_tag ', data[i])\n",
      "        \n",
      "# def preprocess(data):\n",
      "    # PS_WORDS=frozenset(['appliance', 'battery', 'blade', 'bought', 'broken', 'bulb', 'buy', 'car', 'cost', 'dishwasher', 'dispenser', 'door', 'drill', 'dryer', 'filter', 'freezer', 'fridge', 'glass', 'guarantee', 'handle', 'hole','hose', 'ice', 'instruction', 'iron', 'item', 'light', 'machine', 'manual', 'model', 'new', 'order', 'oven', 'problem', 'product', 'purchase', 'refrigerator', 'replace', 'replacement', 'spare', 'spin', 'spring', 'tool', 'used', 'warranty', 'wash', 'washer', 'washing', 'water'])\n",
      "    # HR_WORDS=frozenset(['application', 'apply', 'bachelor', 'business', 'career', 'college', 'cv', 'department', 'design', 'employment', 'engineer', 'engineering', 'experience', 'german', 'germany', 'graduate','growth', 'hr', 'industry','internship', 'job', 'join', 'junior', 'madam', 'management', 'manager', 'master', 'month', 'opening', 'opportunity', 'organisation', 'phd', 'placement', 'position', 'process', 'profile', 'program', 'project', 'recruitment', 'related', 'resource', 'resume', 'skill','student', 'study', 'studying', 'team', 'trainee', 'training', 'university', 'wish'])\n",
      "    # SALUTATION_WORDS=frozenset(['hi','hello','dear','bye','thank', 'thanks','best', 'cheers'])\n",
      "    # PS_regex = re.compile('\\s('+'|'.join(PS_WORDS)+')\\s')\n",
      "    # HR_regex = re.compile('\\s('+'|'.join(HR_WORDS)+')\\s')\n",
      "    # SAL_regex = re.compile('\\s('+'|'.join(HR_WORDS)+')\\s')\n",
      "    # wnl = WordNetLemmatizer()\n",
      "    # for i in range(len(data)):\n",
      "        # data[i]=data[i].lower()\n",
      "        # data[i]=data[i].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "        # data[i]=' '.join([wnl.lemmatize(tok) for tok in data[i].split()])\n",
      "        # data[i]=re.sub('\\s[2][0-1][0-9][0-9]\\s', ' YEAR_tag ', data[i])\n",
      "        # data[i]=re.sub('\\s[0-9]+\\s', ' NUM_tag ', data[i])\n",
      "        # data[i]=re.sub('\\s([0-9]|[a-z])([0-9]|[a-z])+[0-9]([0-9]|[a-z])+\\s', ' SERIAL_tag ', data[i])\n",
      "        # data[i]=re.sub(PS_regex, ' PS_tag ', data[i])\n",
      "        # data[i]=re.sub(HR_regex, ' HR_tag ', data[i])\n",
      "        # data[i]=re.sub(SAL_regex, ' SAL_tag ', data[i])\n",
      "    \n",
      "def fit_vectorizer(data):    \n",
      "    print(\"Creating Vectorizer...\")\n",
      "    t0 = time()\n",
      "    myTokenizer=CleanPorterTokenizer()   # Select the Tokenizer to use\n",
      "    #myTokenizer=CleanLemmaSynsetTokenizer() \n",
      "    #myTokenizer=CleanPorterTokenizer() \n",
      "    vectorizer = TfidfVectorizer(max_features=2000, tokenizer=myTokenizer,\n",
      "        stop_words='english', use_idf=True, binary=False, charset_error='ignore',  max_df=.99)\n",
      "    vectorizer.fit(data)\n",
      "    print(\"done in %fs\" % (time() - t0))\n",
      "    return vectorizer\n",
      "\n",
      "def fit_count_vectorizer(data):\n",
      "    print(\"Creating Count Vectorizer...\")\n",
      "    t0 = time()\n",
      "    myTokenizer = CleanLemmaTokenizer()\n",
      "    vectorizer = CountVectorizer(max_features = 2000, tokenizer = myTokenizer,\n",
      "                                 stop_words = 'english', binary =True, charset_error='ignore', max_df =.99)\n",
      "    vectorizer.fit(data)\n",
      "    print(\"done in %fs\" %(time() - t0))\n",
      "    return vectorizer\n",
      "\n",
      "## *************************    Group Features      *******************\n",
      "def category_features(X, target, target_val, vectorizer):\n",
      "    indices = [i for i, x in enumerate(target) if x == target_val]\n",
      "    X_target_sums=np.sum(X[indices], axis=0)\n",
      "    features=vectorizer.get_feature_names()\n",
      "    # for i, x in sorted(enumerate(X_target_sums), key=lambda tup:tup[1], reverse=True)[:30]:\n",
      "        # print features[i], x\n",
      "    top_features=[i for i,x in sorted(enumerate(X_target_sums), key=lambda tup:tup[1], reverse=True)]\n",
      "    return top_features\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree, objectify\n",
      "from io import StringIO, BytesIO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree = etree.parse(\"14-28-RAW-Solr-5_toparse.xml\")\n",
      "root = tree.getroot()\n",
      "mylist = []\n",
      "for doc in root.findall('doc'):\n",
      "    mylist.append(dict([(i.values()[0], i.getchildren()[0].text) for i in doc.getchildren() if i.values()[0]!='score']))\n",
      "\n",
      "comments = [d['text'] for d in mylist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#string_data     list of strings\n",
      "type(comments),type(comments[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "(list, str)"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = fit_vectorizer(comments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating Vectorizer...\n",
        "done in 166.346281s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/genevievekathleensmith/Documents/insight/env/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:625: DeprecationWarning: The charset_error parameter is deprecated as of version 0.14 and will be removed in 0.16. Use decode_error instead.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data_vec = vectorizer(string_data)\n",
      "vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "TfidfVectorizer(analyzer=u'word', binary=False, charset=None,\n",
        "        charset_error=None, decode_error='ignore',\n",
        "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
        "        lowercase=True, max_df=0.99, max_features=2000, min_df=1,\n",
        "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
        "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
        "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
        "        tokenizer=<__main__.CleanPorterTokenizer object at 0x115094110>,\n",
        "        use_idf=True, vocabulary=None)"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[u'00',\n",
        " u'000',\n",
        " u'01',\n",
        " u'02',\n",
        " u'03',\n",
        " u'04',\n",
        " u'05',\n",
        " u'06',\n",
        " u'07',\n",
        " u'08',\n",
        " u'09',\n",
        " u'10',\n",
        " u'100',\n",
        " u'11',\n",
        " u'12',\n",
        " u'128d',\n",
        " u'12th',\n",
        " u'13',\n",
        " u'14',\n",
        " u'15',\n",
        " u'16',\n",
        " u'17',\n",
        " u'18',\n",
        " u'19',\n",
        " u'1996',\n",
        " u'20',\n",
        " u'2005',\n",
        " u'2006',\n",
        " u'2007',\n",
        " u'2008',\n",
        " u'2009',\n",
        " u'2010',\n",
        " u'2011',\n",
        " u'2012',\n",
        " u'2013',\n",
        " u'2014',\n",
        " u'20554',\n",
        " u'21',\n",
        " u'21f',\n",
        " u'21st',\n",
        " u'22',\n",
        " u'23',\n",
        " u'24',\n",
        " u'25',\n",
        " u'26',\n",
        " u'2601',\n",
        " u'27',\n",
        " u'28',\n",
        " u'29',\n",
        " u'2b4',\n",
        " u'30',\n",
        " u'31',\n",
        " u'32',\n",
        " u'33',\n",
        " u'34',\n",
        " u'35',\n",
        " u'36',\n",
        " u'37',\n",
        " u'38',\n",
        " u'39',\n",
        " u'3d',\n",
        " u'40',\n",
        " u'41',\n",
        " u'42',\n",
        " u'43',\n",
        " u'44',\n",
        " u'445',\n",
        " u'45',\n",
        " u'46',\n",
        " u'47',\n",
        " u'48',\n",
        " u'49',\n",
        " u'50',\n",
        " u'500',\n",
        " u'51',\n",
        " u'52',\n",
        " u'53',\n",
        " u'54',\n",
        " u'55',\n",
        " u'56',\n",
        " u'57',\n",
        " u'58',\n",
        " u'59',\n",
        " u'5bff',\n",
        " u'60',\n",
        " u'61',\n",
        " u'70',\n",
        " u'706',\n",
        " u'75',\n",
        " u'7521156625',\n",
        " u'7521171847',\n",
        " u'7521191158',\n",
        " u'80',\n",
        " u'85',\n",
        " u'90',\n",
        " u'99',\n",
        " u'abandon',\n",
        " u'abil',\n",
        " u'abl',\n",
        " u'abolish',\n",
        " u'abov',\n",
        " u'absolut',\n",
        " u'absurd',\n",
        " u'abus',\n",
        " u'accept',\n",
        " u'access',\n",
        " u'accomplish',\n",
        " u'accord',\n",
        " u'account',\n",
        " u'achiev',\n",
        " u'acknowledg',\n",
        " u'act',\n",
        " u'action',\n",
        " u'activ',\n",
        " u'actual',\n",
        " u'ad',\n",
        " u'adam',\n",
        " u'add',\n",
        " u'addit',\n",
        " u'address',\n",
        " u'adequ',\n",
        " u'adjust',\n",
        " u'administr',\n",
        " u'admit',\n",
        " u'adopt',\n",
        " u'advanc',\n",
        " u'advantag',\n",
        " u'advertis',\n",
        " u'advoc',\n",
        " u'affair',\n",
        " u'affect',\n",
        " u'afford',\n",
        " u'afraid',\n",
        " u'age',\n",
        " u'agenc',\n",
        " u'agenda',\n",
        " u'ago',\n",
        " u'agre',\n",
        " u'agreement',\n",
        " u'ah',\n",
        " u'ahead',\n",
        " u'aid',\n",
        " u'aim',\n",
        " u'air',\n",
        " u'akin',\n",
        " u'al',\n",
        " u'alex',\n",
        " u'alexand',\n",
        " u'alik',\n",
        " u'aliv',\n",
        " u'allow',\n",
        " u'alon',\n",
        " u'alreadi',\n",
        " u'alter',\n",
        " u'altern',\n",
        " u'alway',\n",
        " u'amaz',\n",
        " u'amazon',\n",
        " u'amend',\n",
        " u'america',\n",
        " u'american',\n",
        " u'analog',\n",
        " u'analysi',\n",
        " u'andrew',\n",
        " u'angel',\n",
        " u'angri',\n",
        " u'ani',\n",
        " u'ann',\n",
        " u'anna',\n",
        " u'announc',\n",
        " u'anoth',\n",
        " u'answer',\n",
        " u'anti',\n",
        " u'anybodi',\n",
        " u'anymor',\n",
        " u'anyon',\n",
        " u'anyth',\n",
        " u'anywher',\n",
        " u'apart',\n",
        " u'app',\n",
        " u'appal',\n",
        " u'appar',\n",
        " u'appeal',\n",
        " u'appear',\n",
        " u'appl',\n",
        " u'appli',\n",
        " u'applic',\n",
        " u'appoint',\n",
        " u'appreci',\n",
        " u'approach',\n",
        " u'appropri',\n",
        " u'approv',\n",
        " u'apr',\n",
        " u'april',\n",
        " u'architectur',\n",
        " u'area',\n",
        " u'aren',\n",
        " u'argu',\n",
        " u'argument',\n",
        " u'arm',\n",
        " u'armi',\n",
        " u'arrang',\n",
        " u'arriv',\n",
        " u'art',\n",
        " u'articl',\n",
        " u'artifici',\n",
        " u'artist',\n",
        " u'asham',\n",
        " u'asid',\n",
        " u'ask',\n",
        " u'aspect',\n",
        " u'ass',\n",
        " u'associ',\n",
        " u'assum',\n",
        " u'assur',\n",
        " u'attach',\n",
        " u'attack',\n",
        " u'attempt',\n",
        " u'attent',\n",
        " u'attract',\n",
        " u'au',\n",
        " u'audienc',\n",
        " u'author',\n",
        " u'avail',\n",
        " u'avenu',\n",
        " u'averag',\n",
        " u'avoid',\n",
        " u'aw',\n",
        " u'awar',\n",
        " u'away',\n",
        " u'backbon',\n",
        " u'backward',\n",
        " u'bad',\n",
        " u'balanc',\n",
        " u'ball',\n",
        " u'ban',\n",
        " u'band',\n",
        " u'bandwidth',\n",
        " u'bank',\n",
        " u'bar',\n",
        " u'barbara',\n",
        " u'bare',\n",
        " u'barricad',\n",
        " u'barrier',\n",
        " u'base',\n",
        " u'basi',\n",
        " u'basic',\n",
        " u'bastion',\n",
        " u'battl',\n",
        " u'bear',\n",
        " u'beauti',\n",
        " u'becam',\n",
        " u'becaus',\n",
        " u'becom',\n",
        " u'bed',\n",
        " u'befor',\n",
        " u'beg',\n",
        " u'began',\n",
        " u'begin',\n",
        " u'behalf',\n",
        " u'behavior',\n",
        " u'belief',\n",
        " u'believ',\n",
        " u'bell',\n",
        " u'belong',\n",
        " u'bend',\n",
        " u'benefici',\n",
        " u'benefit',\n",
        " u'besid',\n",
        " u'best',\n",
        " u'better',\n",
        " u'bia',\n",
        " u'bidder',\n",
        " u'big',\n",
        " u'bigger',\n",
        " u'biggest',\n",
        " u'billion',\n",
        " u'bishop',\n",
        " u'bit',\n",
        " u'black',\n",
        " u'blatant',\n",
        " u'blind',\n",
        " u'block',\n",
        " u'blog',\n",
        " u'blood',\n",
        " u'blow',\n",
        " u'board',\n",
        " u'bodi',\n",
        " u'book',\n",
        " u'bori',\n",
        " u'born',\n",
        " u'bought',\n",
        " u'bound',\n",
        " u'bow',\n",
        " u'box',\n",
        " u'boy',\n",
        " u'branch',\n",
        " u'brand',\n",
        " u'break',\n",
        " u'breath',\n",
        " u'brian',\n",
        " u'bribe',\n",
        " u'bridg',\n",
        " u'bring',\n",
        " u'broad',\n",
        " u'broadband',\n",
        " u'broadcast',\n",
        " u'broke',\n",
        " u'broken',\n",
        " u'brother',\n",
        " u'brought',\n",
        " u'brown',\n",
        " u'build',\n",
        " u'built',\n",
        " u'bulli',\n",
        " u'burden',\n",
        " u'burn',\n",
        " u'busi',\n",
        " u'buy',\n",
        " u'ca',\n",
        " u'cabl',\n",
        " u'came',\n",
        " u'campaign',\n",
        " u'cap',\n",
        " u'capabl',\n",
        " u'capac',\n",
        " u'capit',\n",
        " u'capitalist',\n",
        " u'car',\n",
        " u'care',\n",
        " u'career',\n",
        " u'carol',\n",
        " u'carri',\n",
        " u'carriag',\n",
        " u'carrier',\n",
        " u'cart',\n",
        " u'case',\n",
        " u'cash',\n",
        " u'cast',\n",
        " u'cat',\n",
        " u'categori',\n",
        " u'caught',\n",
        " u'caus',\n",
        " u'cave',\n",
        " u'ceas',\n",
        " u'cell',\n",
        " u'censor',\n",
        " u'censorship',\n",
        " u'center',\n",
        " u'central',\n",
        " u'centuri',\n",
        " u'ceo',\n",
        " u'certain',\n",
        " u'certainli',\n",
        " u'chair',\n",
        " u'chairman',\n",
        " u'challeng',\n",
        " u'chanc',\n",
        " u'chang',\n",
        " u'channel',\n",
        " u'chapter',\n",
        " u'charg',\n",
        " u'charl',\n",
        " u'charter',\n",
        " u'check',\n",
        " u'chief',\n",
        " u'child',\n",
        " u'children',\n",
        " u'china',\n",
        " u'choic',\n",
        " u'choke',\n",
        " u'choos',\n",
        " u'chri',\n",
        " u'christoph',\n",
        " u'circuit',\n",
        " u'circumst',\n",
        " u'citi',\n",
        " u'citizen',\n",
        " u'civil',\n",
        " u'claim',\n",
        " u'class',\n",
        " u'classif',\n",
        " u'classifi',\n",
        " u'clear',\n",
        " u'clearli',\n",
        " u'client',\n",
        " u'close',\n",
        " u'cloud',\n",
        " u'coat',\n",
        " u'code',\n",
        " u'cold',\n",
        " u'collabor',\n",
        " u'collect',\n",
        " u'colleg',\n",
        " u'com',\n",
        " u'combin',\n",
        " u'comcast',\n",
        " u'come',\n",
        " u'command',\n",
        " u'comment',\n",
        " u'commerc',\n",
        " u'commerci',\n",
        " u'commiss',\n",
        " u'commission',\n",
        " u'commit',\n",
        " u'committe',\n",
        " u'commod',\n",
        " u'common',\n",
        " u'commun',\n",
        " u'compani',\n",
        " u'compar',\n",
        " u'comparison',\n",
        " u'compet',\n",
        " u'competit',\n",
        " u'competitor',\n",
        " u'complaint',\n",
        " u'complet',\n",
        " u'complex',\n",
        " u'compon',\n",
        " u'compromis',\n",
        " u'comput',\n",
        " u'concept',\n",
        " u'concern',\n",
        " u'conclud',\n",
        " u'conclus',\n",
        " u'condit',\n",
        " u'conduct',\n",
        " u'confid',\n",
        " u'conflict',\n",
        " u'confus',\n",
        " u'congest',\n",
        " u'conglomer',\n",
        " u'congress',\n",
        " u'congression',\n",
        " u'connect',\n",
        " u'consequ',\n",
        " u'consid',\n",
        " u'consider',\n",
        " u'consist',\n",
        " u'consolid',\n",
        " u'constitut',\n",
        " u'consum',\n",
        " u'contact',\n",
        " u'contain',\n",
        " u'content',\n",
        " u'context',\n",
        " u'continu',\n",
        " u'contract',\n",
        " u'contrari',\n",
        " u'contribut',\n",
        " u'control',\n",
        " u'convers',\n",
        " u'convinc',\n",
        " u'cooper',\n",
        " u'copi',\n",
        " u'core',\n",
        " u'corner',\n",
        " u'corp',\n",
        " u'corpor',\n",
        " u'correct',\n",
        " u'corrupt',\n",
        " u'cosett',\n",
        " u'cost',\n",
        " u'couldn',\n",
        " u'count',\n",
        " u'countess',\n",
        " u'countless',\n",
        " u'countri',\n",
        " u'coupl',\n",
        " u'cours',\n",
        " u'court',\n",
        " u'cover',\n",
        " u'cox',\n",
        " u'creat',\n",
        " u'creation',\n",
        " u'creativ',\n",
        " u'creator',\n",
        " u'cri',\n",
        " u'crime',\n",
        " u'crimin',\n",
        " u'crippl',\n",
        " u'criteria',\n",
        " u'critic',\n",
        " u'cross',\n",
        " u'crowd',\n",
        " u'crucial',\n",
        " u'crush',\n",
        " u'ct',\n",
        " u'cultur',\n",
        " u'current',\n",
        " u'custom',\n",
        " u'cut',\n",
        " u'd85',\n",
        " u'daili',\n",
        " u'damag',\n",
        " u'dan',\n",
        " u'danger',\n",
        " u'daniel',\n",
        " u'dare',\n",
        " u'dark',\n",
        " u'data',\n",
        " u'date',\n",
        " u'daughter',\n",
        " u'dave',\n",
        " u'david',\n",
        " u'day',\n",
        " u'dc',\n",
        " u'dead',\n",
        " u'deal',\n",
        " u'dear',\n",
        " u'death',\n",
        " u'debat',\n",
        " u'decad',\n",
        " u'decent',\n",
        " u'decid',\n",
        " u'decis',\n",
        " u'declar',\n",
        " u'declin',\n",
        " u'decreas',\n",
        " u'deem',\n",
        " u'deep',\n",
        " u'deepli',\n",
        " u'defend',\n",
        " u'defens',\n",
        " u'defin',\n",
        " u'definit',\n",
        " u'degrad',\n",
        " u'degre',\n",
        " u'delay',\n",
        " u'deliv',\n",
        " u'deliveri',\n",
        " u'demand',\n",
        " u'democraci',\n",
        " u'democrat',\n",
        " u'demonstr',\n",
        " u'deni',\n",
        " u'denisov',\n",
        " u'depart',\n",
        " u'depend',\n",
        " u'deploy',\n",
        " u'deregul',\n",
        " u'describ',\n",
        " u'deserv',\n",
        " u'design',\n",
        " u'desir',\n",
        " u'despit',\n",
        " u'destin',\n",
        " u'destroy',\n",
        " u'destruct',\n",
        " u'determin',\n",
        " u'detriment',\n",
        " u'develop',\n",
        " u'devic',\n",
        " u'dial',\n",
        " u'dictat',\n",
        " u'did',\n",
        " u'didn',\n",
        " u'die',\n",
        " u'differ',\n",
        " u'differenti',\n",
        " u'difficult',\n",
        " u'digit',\n",
        " u'diminish',\n",
        " u'dip',\n",
        " u'direct',\n",
        " u'directli',\n",
        " u'disadvantag',\n",
        " u'disagre',\n",
        " u'disappear',\n",
        " u'disappoint',\n",
        " u'disast',\n",
        " u'disastr',\n",
        " u'disclosur',\n",
        " u'discourag',\n",
        " u'discret',\n",
        " u'discrimin',\n",
        " u'discriminatori',\n",
        " u'discuss',\n",
        " u'disgrac',\n",
        " u'disgust',\n",
        " u'disput',\n",
        " u'disrupt',\n",
        " u'distanc',\n",
        " u'distinct',\n",
        " u'distribut',\n",
        " u'district',\n",
        " u'disturb',\n",
        " u'divers',\n",
        " u'divid',\n",
        " u'docket',\n",
        " u'doctor',\n",
        " u'document',\n",
        " u'doe',\n",
        " u'doesn',\n",
        " u'dog',\n",
        " u'dollar',\n",
        " u'dolokhov',\n",
        " u'domain',\n",
        " u'domin',\n",
        " u'don',\n",
        " u'dont',\n",
        " u'door',\n",
        " u'doubl',\n",
        " u'doubt',\n",
        " u'download',\n",
        " u'draw',\n",
        " u'dream',\n",
        " u'dress',\n",
        " u'drew',\n",
        " u'drive',\n",
        " u'driven',\n",
        " u'drop',\n",
        " u'dsl',\n",
        " u'dure',\n",
        " u'duti',\n",
        " u'ear',\n",
        " u'earli',\n",
        " u'earn',\n",
        " u'earth',\n",
        " u'easi',\n",
        " u'easier',\n",
        " u'easili',\n",
        " u'eat',\n",
        " u'eb',\n",
        " u'ec',\n",
        " u'econom',\n",
        " u'economi',\n",
        " u'ed',\n",
        " u'edg',\n",
        " u'educ',\n",
        " u'edward',\n",
        " u'effect',\n",
        " u'effici',\n",
        " u'effort',\n",
        " u'egregi',\n",
        " u'elect',\n",
        " u'electr',\n",
        " u'electron',\n",
        " u'elimin',\n",
        " u'els',\n",
        " u'email',\n",
        " u'embarrass',\n",
        " u'emerg',\n",
        " u'emperor',\n",
        " u'employ',\n",
        " u'employe',\n",
        " u'empow',\n",
        " u'en',\n",
        " u'enabl',\n",
        " u'enact',\n",
        " u'encourag',\n",
        " u'end',\n",
        " u'enemi',\n",
        " u'enforc',\n",
        " u'engag',\n",
        " u'engin',\n",
        " u'enhanc',\n",
        " u'enjoy',\n",
        " u'enorm',\n",
        " u'enrich',\n",
        " u'ensur',\n",
        " u'enter',\n",
        " u'enterpris',\n",
        " u'entertain',\n",
        " u'entir',\n",
        " u'entiti',\n",
        " u'entrench',\n",
        " u'entrepreneur',\n",
        " u'entrepreneurship',\n",
        " u'entri',\n",
        " u'environ',\n",
        " u'equal',\n",
        " u'equip',\n",
        " u'equival',\n",
        " u'er',\n",
        " u'era',\n",
        " u'eric',\n",
        " u'es',\n",
        " u'escap',\n",
        " u'especi',\n",
        " u'essenti',\n",
        " u'establish',\n",
        " u'et',\n",
        " u'europ',\n",
        " u'european',\n",
        " u'event',\n",
        " u'eventu',\n",
        " u'everi',\n",
        " u'everybodi',\n",
        " u'everyday',\n",
        " u'everyon',\n",
        " u'everyth',\n",
        " u'everywher',\n",
        " u'evid',\n",
        " u'evil',\n",
        " u'evolv',\n",
        " u'ex',\n",
        " u'exact',\n",
        " u'exactli',\n",
        " u'examin',\n",
        " u'exampl',\n",
        " u'excel',\n",
        " u'exchang',\n",
        " u'exclaim',\n",
        " u'exclus',\n",
        " u'excus',\n",
        " u'execut',\n",
        " u'exercis',\n",
        " u'exist',\n",
        " u'expand',\n",
        " u'expans',\n",
        " u'expect',\n",
        " u'expens',\n",
        " u'experi',\n",
        " u'experienc',\n",
        " u'explain',\n",
        " u'exploit',\n",
        " u'explor',\n",
        " u'express',\n",
        " u'extend',\n",
        " u'extens',\n",
        " u'extent',\n",
        " u'extort',\n",
        " u'extra',\n",
        " u'extract',\n",
        " u'extrem',\n",
        " u'eye',\n",
        " u'face',\n",
        " u'facebook',\n",
        " u'facil',\n",
        " u'facilit',\n",
        " u'fact',\n",
        " u'facto',\n",
        " u'factor',\n",
        " u'fail',\n",
        " u'failur',\n",
        " u'fair',\n",
        " u'fairli',\n",
        " u'faith',\n",
        " u'fall',\n",
        " u'fals',\n",
        " u'famili',\n",
        " u'far',\n",
        " u'fashion',\n",
        " u'fast',\n",
        " u'faster',\n",
        " u'fastest',\n",
        " u'father',\n",
        " u'fauchelev',\n",
        " u'favor',\n",
        " u'favorit',\n",
        " u'fax',\n",
        " u'fcc',\n",
        " u'fear',\n",
        " u'featur',\n",
        " u'feder',\n",
        " u'fee',\n",
        " u'feel',\n",
        " u'feet',\n",
        " u'fef1',\n",
        " u'fell',\n",
        " u'fellow',\n",
        " u'felt',\n",
        " u'fewer',\n",
        " u'fi',\n",
        " u'fiber',\n",
        " u'field',\n",
        " u'fight',\n",
        " u'figur',\n",
        " u'file',\n",
        " u'filter',\n",
        " u'final',\n",
        " u'financi',\n",
        " u'fine',\n",
        " u'finger',\n",
        " u'finish',\n",
        " u'firewal',\n",
        " u'firm',\n",
        " u'firmli',\n",
        " u'fit',\n",
        " u'fix',\n",
        " u'flexibl',\n",
        " u'fli',\n",
        " u'flourish',\n",
        " u'flow',\n",
        " u'focu',\n",
        " u'focus',\n",
        " u'folk',\n",
        " u'follow',\n",
        " u'food',\n",
        " u'foot',\n",
        " u'forc',\n",
        " u'foreign',\n",
        " u'forev',\n",
        " u'forget',\n",
        " u'forgotten',\n",
        " u'form',\n",
        " u'formal',\n",
        " u'forth',\n",
        " u'fortun',\n",
        " u'forum',\n",
        " u'forward',\n",
        " u'foster',\n",
        " u'foundat',\n",
        " u'fox',\n",
        " u'fr',\n",
        " u'framework',\n",
        " u'franc',\n",
        " u'frankli',\n",
        " u'free',\n",
        " u'freedom',\n",
        " u'freeli',\n",
        " u'french',\n",
        " u'frequent',\n",
        " u'friend',\n",
        " u'frighten',\n",
        " u'frodo',\n",
        " u'fuck',\n",
        " u'fuel',\n",
        " u'fulli',\n",
        " u'function',\n",
        " u'fund',\n",
        " u'fundament',\n",
        " u'furthermor',\n",
        " u'futur',\n",
        " u'g5',\n",
        " u'ga',\n",
        " u'gain',\n",
        " u'game',\n",
        " u'gandalf',\n",
        " u'gap',\n",
        " u'garden',\n",
        " u'gari',\n",
        " u'gate',\n",
        " u'gatekeep',\n",
        " u'gave',\n",
        " u'gavroch',\n",
        " u'gaze',\n",
        " u'gb',\n",
        " u'gener',\n",
        " u'georg',\n",
        " u'giant',\n",
        " u'gift',\n",
        " u'girl',\n",
        " u'given',\n",
        " u'glanc',\n",
        " u'global',\n",
        " u'gn',\n",
        " u'goal',\n",
        " u'god',\n",
        " u'goe',\n",
        " u'gone',\n",
        " u'good',\n",
        " u'googl',\n",
        " u'got',\n",
        " u'goug',\n",
        " u'gov',\n",
        " u'govern',\n",
        " u'grab',\n",
        " u'grand',\n",
        " u'grant',\n",
        " u'grave',\n",
        " u'great',\n",
        " u'greater',\n",
        " u'greatest',\n",
        " u'greatli',\n",
        " u'greed',\n",
        " u'greedi',\n",
        " u'green',\n",
        " u'grew',\n",
        " u'gross',\n",
        " u'ground',\n",
        " u'group',\n",
        " u'grow',\n",
        " u'grown',\n",
        " u'growth',\n",
        " u'guarante',\n",
        " u'guard',\n",
        " u'guess',\n",
        " u'guid',\n",
        " u'gun',\n",
        " u'guy',\n",
        " u'ha',\n",
        " u'hair',\n",
        " u'half',\n",
        " u'hall',\n",
        " u'halt',\n",
        " u'hamper',\n",
        " u'hand',\n",
        " u'handl',\n",
        " u'happen',\n",
        " u'happi',\n",
        " u'hard',\n",
        " u'harder',\n",
        " u'hardli',\n",
        " u'harm',\n",
        " u'hate',\n",
        " u'haven',\n",
        " u'head',\n",
        " u'health',\n",
        " u'healthi',\n",
        " u'hear',\n",
        " u'heard',\n",
        " u'heart',\n",
        " u'heavi',\n",
        " u'heavili',\n",
        " u'held',\n",
        " u'helen',\n",
        " u'hell',\n",
        " u'hello',\n",
        " u'help',\n",
        " u'hi',\n",
        " u'high',\n",
        " u'higher',\n",
        " u'highest',\n",
        " u'highli',\n",
        " u'highway',\n",
        " u'hill',\n",
        " u'hinder',\n",
        " u'hire',\n",
        " u'histor',\n",
        " u'histori',\n",
        " u'hold',\n",
        " u'hole',\n",
        " u'home',\n",
        " u'honest',\n",
        " u'honestli',\n",
        " u'honor',\n",
        " u'hope',\n",
        " u'horribl',\n",
        " u'hors',\n",
        " u'host',\n",
        " u'hostag',\n",
        " u'hour',\n",
        " u'hous',\n",
        " u'household',\n",
        " u'howev',\n",
        " u'http',\n",
        " u'huge',\n",
        " u'hulu',\n",
        " u'human',\n",
        " u'hundr',\n",
        " u'hurt',\n",
        " u'husband',\n",
        " u'ic',\n",
        " u'id',\n",
        " u'idea',\n",
        " u'ideal',\n",
        " u'identifi',\n",
        " u'idiot',\n",
        " u'ignor',\n",
        " u'ii',\n",
        " u'iii',\n",
        " u'ill',\n",
        " u'illeg',\n",
        " u'im',\n",
        " u'imac',\n",
        " u'imagin',\n",
        " u'immedi',\n",
        " u'immens',\n",
        " u'impact',\n",
        " u'imped',\n",
        " u'imper',\n",
        " u'implement',\n",
        " u'implic',\n",
        " u'implor',\n",
        " u'import',\n",
        " u'importantli',\n",
        " u'impos',\n",
        " u'imposs',\n",
        " u'improv',\n",
        " u'incent',\n",
        " u'includ',\n",
        " u'incom',\n",
        " u'increas',\n",
        " u'increasingli',\n",
        " u'incred',\n",
        " u'incumb',\n",
        " u'inde',\n",
        " u'independ',\n",
        " u'indic',\n",
        " u'individu',\n",
        " u'industri',\n",
        " u'inequ',\n",
        " u'inevit',\n",
        " u'influenc',\n",
        " u'inform',\n",
        " u'infrastructur',\n",
        " u'infring',\n",
        " u'inher',\n",
        " u'inhibit',\n",
        " u'initi',\n",
        " u'innov',\n",
        " u'inquiri',\n",
        " u'insan',\n",
        " u'insid',\n",
        " u'instal',\n",
        " u'instanc',\n",
        " u'instant',\n",
        " u'instead',\n",
        " ...]"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term_matrix = vectorizer.transform(comments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "<100000x2000 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 3007292 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cosine_similarity(term_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cosine_similarity(term_matrix[0,],term_matrix[10000,])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "array([[ 0.16841435]])"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "similarity_matrix = cosine_similarity(term_matrix[0:10000,])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(similarity_matrix), size(similarity_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "(numpy.ndarray, 100000000)"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#np.savetxt(\"sim_matrix_10000.csv\", similarity_matrix, delimiter=\",\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sim_mat = pd.read_csv('sim_matrix_10000.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#sim_mat"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "states = [d['stateCd'] if 'stateCd' in d else None for d in mylist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#states"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(states)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "list"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#data = similarity_matrix.toarray()\n",
      "\n",
      "# This gives a matrix, rows are comments, columns are word occurances\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-41-d3c70e791ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# This gives a matrix, rows are comments, columns are word occurances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'toarray'"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_svd = svd(similarity_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}