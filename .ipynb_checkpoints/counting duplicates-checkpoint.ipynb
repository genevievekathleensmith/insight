{
 "metadata": {
  "name": "",
  "signature": "sha256:2ea03f004d2c889ac51fad0bd2be508664ba03ccc171d5421b2e3e7d56b6b925"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from lxml import etree, objectify\n",
      "from io import StringIO, BytesIO"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
      "from time import time\n",
      "import os, os.path\n",
      "import csv\n",
      "from bunch import Bunch\n",
      "import numpy as np\n",
      "import re\n",
      "import string\n",
      "# For strip/clean data. Replace words w/root form, \"clean\" rids html\n",
      "from nltk.corpus import wordnet as wn\n",
      "from nltk import word_tokenize, regexp_tokenize, clean_html\n",
      "from nltk.stem import WordNetLemmatizer\n",
      "from nltk.stem import PorterStemmer\n",
      "#from langid import classify\n",
      "from sklearn.datasets import load_files\n",
      "import pandas as pd\n",
      "from sklearn.metrics.pairwise import cosine_similarity  \n",
      "from nltk.stem.snowball import SnowballStemmer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "## *************************     Tokenizer      *******************************\n",
      "# for a list of strings\n",
      "class CleanLemmaTokenizer(object):  # Returns root form, AND cleans HTML\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.wnl.lemmatize(t) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "        \n",
      "class SynsetReplacer(object):\n",
      "    def replace(self, word, pos=None):\n",
      "        synset=wn.synsets(word)  #Ordered by commonality, so 1st is best\n",
      "        if len(synset) >= 1:\n",
      "            return synset[0].name  ## first synonym is the most common\n",
      "        else:\n",
      "            return word   \n",
      "            \n",
      "class CleanLemmaSynsetTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.wnl = WordNetLemmatizer()\n",
      "        self.replacer = SynsetReplacer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.replacer.replace(self.wnl.lemmatize(t)) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "        \n",
      "class CleanPorterTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.ps = PorterStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.ps.stem(t) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "\n",
      "class CleanSnowballTokenizer(object):\n",
      "    def __init__(self):\n",
      "        self.ps = SnowballStemmer()\n",
      "    def __call__(self, doc):\n",
      "        return [self.ps.stem(t) for t in regexp_tokenize(clean_html(doc), '\\w\\w+')]\n",
      "\n",
      "## *************************    VECTORIZE DATA      *******************************\n",
      "# FOR GETTING Feature words: print '\\', \\''.join([features[i] for i in PS]) \n",
      "def preprocess_num(data):\n",
      "    for i in range(len(data)):\n",
      "        data[i]=data[i].lower()\n",
      "        #data[i]=data[i].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "        data[i]=re.sub('\\s[2][0-1][0-9][0-9]\\s', ' YEAR_tag ', data[i])\n",
      "        data[i]=re.sub('\\s[0-9]+\\s', ' NUM_tag ', data[i])\n",
      "        data[i]=re.sub('\\s([0-9]|[a-z])([0-9]|[a-z])+[0-9]([0-9]|[a-z])+\\s', ' SERIAL_tag ', data[i])\n",
      "        \n",
      "# def preprocess(data):\n",
      "    # PS_WORDS=frozenset(['appliance', 'battery', 'blade', 'bought', 'broken', 'bulb', 'buy', 'car', 'cost', 'dishwasher', 'dispenser', 'door', 'drill', 'dryer', 'filter', 'freezer', 'fridge', 'glass', 'guarantee', 'handle', 'hole','hose', 'ice', 'instruction', 'iron', 'item', 'light', 'machine', 'manual', 'model', 'new', 'order', 'oven', 'problem', 'product', 'purchase', 'refrigerator', 'replace', 'replacement', 'spare', 'spin', 'spring', 'tool', 'used', 'warranty', 'wash', 'washer', 'washing', 'water'])\n",
      "    # HR_WORDS=frozenset(['application', 'apply', 'bachelor', 'business', 'career', 'college', 'cv', 'department', 'design', 'employment', 'engineer', 'engineering', 'experience', 'german', 'germany', 'graduate','growth', 'hr', 'industry','internship', 'job', 'join', 'junior', 'madam', 'management', 'manager', 'master', 'month', 'opening', 'opportunity', 'organisation', 'phd', 'placement', 'position', 'process', 'profile', 'program', 'project', 'recruitment', 'related', 'resource', 'resume', 'skill','student', 'study', 'studying', 'team', 'trainee', 'training', 'university', 'wish'])\n",
      "    # SALUTATION_WORDS=frozenset(['hi','hello','dear','bye','thank', 'thanks','best', 'cheers'])\n",
      "    # PS_regex = re.compile('\\s('+'|'.join(PS_WORDS)+')\\s')\n",
      "    # HR_regex = re.compile('\\s('+'|'.join(HR_WORDS)+')\\s')\n",
      "    # SAL_regex = re.compile('\\s('+'|'.join(HR_WORDS)+')\\s')\n",
      "    # wnl = WordNetLemmatizer()\n",
      "    # for i in range(len(data)):\n",
      "        # data[i]=data[i].lower()\n",
      "        # data[i]=data[i].translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "        # data[i]=' '.join([wnl.lemmatize(tok) for tok in data[i].split()])\n",
      "        # data[i]=re.sub('\\s[2][0-1][0-9][0-9]\\s', ' YEAR_tag ', data[i])\n",
      "        # data[i]=re.sub('\\s[0-9]+\\s', ' NUM_tag ', data[i])\n",
      "        # data[i]=re.sub('\\s([0-9]|[a-z])([0-9]|[a-z])+[0-9]([0-9]|[a-z])+\\s', ' SERIAL_tag ', data[i])\n",
      "        # data[i]=re.sub(PS_regex, ' PS_tag ', data[i])\n",
      "        # data[i]=re.sub(HR_regex, ' HR_tag ', data[i])\n",
      "        # data[i]=re.sub(SAL_regex, ' SAL_tag ', data[i])\n",
      "    \n",
      "def fit_vectorizer(data):    \n",
      "    print(\"Creating Vectorizer...\")\n",
      "    t0 = time()\n",
      "    myTokenizer=CleanPorterTokenizer()   # Select the Tokenizer to use\n",
      "    #myTokenizer=CleanLemmaSynsetTokenizer() \n",
      "    #myTokenizer=CleanPorterTokenizer() \n",
      "    vectorizer = TfidfVectorizer(max_features=2000, tokenizer=myTokenizer,\n",
      "        stop_words='english', use_idf=True, binary=False, charset_error='ignore',  max_df=.99)\n",
      "    vectorizer.fit(data)\n",
      "    print(\"done in %fs\" % (time() - t0))\n",
      "    return vectorizer\n",
      "\n",
      "def fit_count_vectorizer(data):\n",
      "    print(\"Creating Count Vectorizer...\")\n",
      "    t0 = time()\n",
      "    myTokenizer = CleanLemmaTokenizer()\n",
      "    vectorizer = CountVectorizer(max_features = 2000, tokenizer = myTokenizer,\n",
      "                                 stop_words = 'english', binary =True, charset_error='ignore', max_df =.99)\n",
      "    vectorizer.fit(data)\n",
      "    print(\"done in %fs\" %(time() - t0))\n",
      "    return vectorizer\n",
      "\n",
      "## *************************    Group Features      *******************\n",
      "def category_features(X, target, target_val, vectorizer):\n",
      "    indices = [i for i, x in enumerate(target) if x == target_val]\n",
      "    X_target_sums=np.sum(X[indices], axis=0)\n",
      "    features=vectorizer.get_feature_names()\n",
      "    # for i, x in sorted(enumerate(X_target_sums), key=lambda tup:tup[1], reverse=True)[:30]:\n",
      "        # print features[i], x\n",
      "    top_features=[i for i,x in sorted(enumerate(X_target_sums), key=lambda tup:tup[1], reverse=True)]\n",
      "    return top_features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree = etree.parse(\"14-28-RAW-Solr-5_toparse.xml\")\n",
      "root = tree.getroot()\n",
      "mylist = []\n",
      "for doc in root.findall('doc'):\n",
      "    mylist.append(dict([(i.values()[0], i.getchildren()[0].text) for i in doc.getchildren() if i.values()[0]!='score']))\n",
      "\n",
      "comments = [d['text'] for d in mylist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree = etree.parse(\"14-28-RAW-Solr-4_toparse.xml\")\n",
      "root = tree.getroot()\n",
      "\n",
      "for doc in root.findall('doc'):\n",
      "    mylist.append(dict([(i.values()[0], i.getchildren()[0].text) for i in doc.getchildren() if i.values()[0]!='score']))\n",
      "\n",
      "comments = [d['text'] for d in mylist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(comments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "146719"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tree = etree.parse(\"14-28-RAW-Solr-1_toparse.xml\")\n",
      "root = tree.getroot()\n",
      "\n",
      "for doc in root.findall('doc'):\n",
      "    mylist.append(dict([(i.values()[0], i.getchildren()[0].text) for i in doc.getchildren() if i.values()[0]!='score']))\n",
      "\n",
      "comments = [d['text'] for d in mylist]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = fit_vectorizer(comments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term_matrix = vectorizer.transform(comments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dense = term_matrix.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dense_df = pd.DataFrame(dense)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unique_matrix = dense_df.drop_duplicates()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shape(unique_matrix)[0],shape(term_matrix)[0],float(shape(unique_matrix)[0])/float(shape(term_matrix)[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "(127825, 246719, 0.5180995383411898)"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shape(term_matrix)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "(246719, 2000)"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "shape(dense)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "(246719, 2000)"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "term_matrix[0,]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "<1x2000 sparse matrix of type '<type 'numpy.float64'>'\n",
        "\twith 92 stored elements in Compressed Sparse Row format>"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dense.max()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "1.0"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "duplication = dense_df.duplicated()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(duplication)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 51,
       "text": [
        "pandas.core.series.Series"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "duplication.to_csv('duplication_status.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "states = [d['stateCd'] if 'stateCd' in d else None for d in mylist]\n",
      "\n",
      "state_array = np.array(states)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(states)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "246719"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "state_array.tofile('state_ids.csv',sep=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "state_series = pd.Series(states)\n",
      "#state_series"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dupes = pd.DataFrame(state_series,duplication)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "size(dupes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 63,
       "text": [
        "246719"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dupes.columns = ['state']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dupes.dupe = dupes.index\n",
      "#groups = dupes.groupby('state',dupes.index)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dupes.index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 78,
       "text": [
        "Index([False, True, True, False, True, True, False, False, False, True, True, True, False, True, True, False, True, True, False, False, True, True, True, False, True, False, True, True, True, False, True, True, False, True, True, True, True, True, False, True, False, True, True, True, True, False, True, True, True, True, True, True, False, True, False, False, True, True, True, False, False, True, True, True, True, True, True, True, False, True, False, True, True, True, False, True, True, False, False, True, True, True, False, True, True, False, True, True, True, True, False, True, False, False, True, False, True, False, True, False, ...], dtype='object')"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}